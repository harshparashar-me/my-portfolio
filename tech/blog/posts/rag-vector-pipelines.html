<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0"/>

<title>RAG Pipelines with Vector Embeddings | Harsh Parashar</title>
<meta name="description" content="A practical guide to building Retrieval-Augmented Generation pipelines using vector databases and embeddings.">

<script src="https://cdn.tailwindcss.com"></script>
<link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;700&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet">

<style>
body { background:#020202; color:#e5e5e5; }
</style>
</head>

<body class="font-mono">

<article class="max-w-3xl mx-auto px-4 py-24">

<a href="/tech/blog/" class="text-purple-400 text-xs tracking-widest">
← ALL LOGS
</a>

<h1 class="text-4xl md:text-5xl font-bold mt-6 mb-6 font-['Space_Grotesk']">
Building RAG Pipelines with Vector Embeddings
</h1>

<p class="text-gray-400 text-sm mb-10">
Published · 08 December 2024 · AI Engineering
</p>

<section class="space-y-6 text-gray-300 leading-relaxed">

<p>
Large Language Models are powerful, but fundamentally limited. They do not remember
your data, your documents, or your business context.
</p>

<p>
This is where <strong>Retrieval-Augmented Generation (RAG)</strong> becomes essential.
Instead of forcing models to hallucinate, we retrieve relevant knowledge and inject it
directly into the prompt.
</p>

<h2 class="text-2xl text-purple-400 mt-10">The Core Problem</h2>

<p>
LLMs are static at inference time. Even fine-tuned models lack awareness of:
</p>

<ul class="list-disc list-inside">
  <li>Private documents</li>
  <li>Frequently changing data</li>
  <li>Domain-specific knowledge</li>
</ul>

<p>
RAG solves this by separating <em>knowledge storage</em> from <em>language reasoning</em>.
</p>

<h2 class="text-2xl text-purple-400 mt-10">How Vector Embeddings Enable Retrieval</h2>

<p>
Instead of keyword matching, RAG relies on semantic similarity.
Text is converted into dense vectors using embedding models.
</p>

<pre class="bg-black border border-white/10 p-4 text-sm overflow-x-auto">
"User authentication flow" → [0.021, -0.332, 0.812, ...]
</pre>

<p>
These vectors are stored in a vector database such as Pinecone, Weaviate, or FAISS.
At query time, the nearest vectors are retrieved using cosine similarity.
</p>

<h2 class="text-2xl text-purple-400 mt-10">A Practical RAG Architecture</h2>

<ul class="list-decimal list-inside">
  <li>Chunk documents into semantically meaningful blocks</li>
  <li>Generate embeddings for each chunk</li>
  <li>Store vectors with metadata</li>
  <li>Embed user query</li>
  <li>Retrieve top-K similar chunks</li>
  <li>Inject into LLM prompt</li>
</ul>

<h2 class="text-2xl text-purple-400 mt-10">Why This Scales Better Than Fine-Tuning</h2>

<p>
Fine-tuning is expensive, slow, and brittle.
RAG allows:
</p>

<ul class="list-disc list-inside">
  <li>Instant updates</li>
  <li>Lower inference cost</li>
  <li>Clear data boundaries</li>
</ul>

<p>
In production systems, RAG is not optional—it is the default architecture.
</p>

</section>

</article>

<footer class="border-t border-white/10 py-6 text-center text-xs text-gray-600">
© Harsh Parashar
</footer>

</body>
</html>
